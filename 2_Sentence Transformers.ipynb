{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpaCy Sentence Transformers\n",
    "\n",
    "Creating a spaCy pipeline that includes a transformer-based sentence embedding component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in a single organization's corpus\n",
    "df = pd.read_json(\"data/02. Data Sets/NIFA/contradictions_datasets_nifa_reports.zip\", orient='records', compression='infer')\n",
    "df['fulltext'] = df.text_by_page.str.join(' ')\n",
    "d0 = df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('senter', <spacy.pipeline.senter.SentenceRecognizer object at 0xffff66de3820>)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "613"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just using this model for its sentence splitting. Everything else disabled.\n",
    "nlp_sentencer = spacy.load('en_core_web_sm')\n",
    "nlp_sentencer.enable_pipe('senter')\n",
    "nlp_sentencer.select_pipes(enable='senter')\n",
    "print(nlp_sentencer.pipeline)\n",
    "\n",
    "# Turn our text into a spacy `doc` which now contains split sentences `sents`\n",
    "d0_doc = nlp_sentencer(d0.fulltext)\n",
    "d0_sents = [sent for sent in d0_doc.sents]\n",
    "len(d0_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sentence_bert',\n",
       "  <spacy_sentence_bert.language.SentenceBert at 0xffff618e3b50>),\n",
       " ('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0xffff614d0440>)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a baseline model to test embedding our sentences using transformers! In\n",
    "# the future better models should be used, this is just to test.\n",
    "#\n",
    "# This will take a moment to download.\n",
    "import spacy_sentence_bert\n",
    "nlp_trf = spacy_sentence_bert.load_model('en_nli_bert_base')\n",
    "nlp_trf.components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sentence_bert',\n",
       "  <spacy_sentence_bert.language.SentenceBert at 0xffff618e3b50>)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The model includes a Sentencizer (rule-based), but we used a\n",
    "# SentenceRecognizer (model-based) already, so we can disable it.\n",
    "nlp_trf.select_pipes(enable='sentence_bert')\n",
    "nlp_trf.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the text of our sentences into spacy `docs` which now contain vectors\n",
    "d0_sent_docs = list(nlp_trf.pipe(sent.text for sent in d0_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is not delinquent on any Federal debt, pursuant to OMB Circular No. A-129, \"Managing Federal Credit Programs,\" and requirements contained in OMB Memorandum —87-32, as implemented by 7 CFR Part 3. i. It will make a good-faith effort to provide and maintain a drug-free environment by prohibiting illicit drugs in the workplace, providing employees with drug-free policy statements (including penalties for noncompliance), and establishing necessary awareness programs to keep employees informed about the availability of counseling, rehabilitation, and related services (§5151-5610 of the Drug-Free Workplace Act of 1988, as implemented by 7 CFR Part 3017, Subpart F).\n",
      "(768,)\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "s = d0_sent_docs[100]\n",
    "print(s.text)\n",
    "print(s.vector.shape)\n",
    "print(s.ents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "premise = \"The office is responsible for writing a report on the project status within 6 months.\"\n",
    "hypothesis_contradict = \"Interim project status reports are not required.\"\n",
    "hypothesis_entail = \"Interim project status reports will be required.\"\n",
    "hypothesis_neutral = \"Operations—activities or processes associated with the programs to be housed in a completed facility and those processes which are necessary to run the facility.\"\n",
    "\n",
    "d_prem, d_cont, d_ent, d_neu = list(nlp_trf.pipe([premise, hypothesis_contradict, hypothesis_entail, hypothesis_neutral]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contradiction Similarity: 0.13807614532548218\n",
      "Entailment Similarity: 0.41477016096460345\n",
      "Neutral Similarity: 0.46119326156374896\n"
     ]
    }
   ],
   "source": [
    "print(\"Contradiction Similarity:\", d_prem.similarity(d_cont))\n",
    "print(\"Entailment Similarity:\", d_prem.similarity(d_ent))\n",
    "print(\"Neutral Similarity:\", d_prem.similarity(d_neu))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Notes**\n",
    "> * We can mix and match pipeline components. So if we wanted to build some algorithm that utilizes sentence embeddings and also looks at the Named Entities in the text, we can use the NER component from another spacy model and just shove in this sentence transformer component!\n",
    "> * The sentence splitting is still wonky. We'll need to do some more cleaning of that functionality\n",
    ">   * For example, our senter creates some sentences that are just a single number or a heading.\n",
    ">   * Simple start could be to clean sentences under a certain length. \n",
    "> * There may be some merit to similarity scores... but I think we're still way too likely to get very similar but contradictory sentences, and very dissimilar but completely unrelated (neutral) sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
