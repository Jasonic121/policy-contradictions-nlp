{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Page Similarity then Sentence Contradictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in a single organization's corpus\n",
    "df = pd.read_json(\"data/02. Data Sets/DoD Issuances/contradictions_datasets_dod_issuances.zip\", orient='records', compression='infer')\n",
    "df['fulltext'] = df.text_by_page.str.join(' ')\n",
    "d0 = df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.esd.whs.mil/Portals/54/Documents/DD/issuances/140025/140025_vol1408.PDF?ver=vMpiDaPFuvIp_30LRxh7OQ%3d%3d'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[16].url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text by page\n",
    "# --> Page embeddings\n",
    "# --> Cosine similarity between all pages\n",
    "# --> For similar pages\n",
    "# --> Split into sentences and run neural contradiction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file_name                                                        100\n",
       "title                                             General Provisions\n",
       "num                                                              100\n",
       "id                 920aeda50488770c12ced1c8d6a7b99be703194f494ce7...\n",
       "corpus                                                 dod_issuances\n",
       "source_page_url        https://www.esd.whs.mil/DD/DoD-Issuances/DTM/\n",
       "url                https://www.esd.whs.mil/Portals/54/Documents/D...\n",
       "type                                                             pdf\n",
       "n_pages                                                            7\n",
       "word_count                                                      1200\n",
       "text_by_page       [othe he Department of Defense INSTRUCTION NUM...\n",
       "fulltext           othe he Department of Defense INSTRUCTION NUMB...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m686.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/site-packages (from en-core-web-sm==3.5.0) (3.5.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/vscode/.local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.23.5)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.4.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.2)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE PAGE EMBEDDINGS\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "pages_docs = list(nlp.pipe(d0.text_by_page))\n",
    "p0 = pages_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.05834313e-02, -2.95698136e-01, -2.07711205e-01,  8.23469460e-02,\n",
       "        2.36313403e-01,  5.06467700e-01,  2.05171406e-01,  4.43163455e-01,\n",
       "        1.81785971e-01,  3.95017751e-02, -5.70735056e-03,  1.00936845e-01,\n",
       "       -5.25057316e-01, -2.93719620e-01, -1.69553727e-01,  1.13907084e-01,\n",
       "       -2.86470741e-01, -2.29209615e-03, -1.14279449e-01, -1.91979527e-01,\n",
       "       -2.62902439e-01, -5.65309860e-02,  2.34655797e-01,  3.48747894e-02,\n",
       "        5.39177842e-02,  4.39624824e-02,  4.77332860e-01,  3.26507628e-01,\n",
       "        3.44411194e-01,  1.06265292e-01, -1.71855748e-01, -2.26183355e-01,\n",
       "        2.85653621e-01, -2.93031940e-03,  9.21946857e-03,  1.70437545e-01,\n",
       "        4.04932678e-01,  1.59191862e-01, -3.29580277e-01, -1.52372420e-01,\n",
       "       -3.07646275e-01, -1.49332851e-01, -2.64494300e-01,  8.02048668e-02,\n",
       "       -1.43276259e-01,  9.34018567e-02,  9.10365209e-02, -6.10388666e-02,\n",
       "        5.06417342e-02,  1.72732212e-02, -2.81670779e-01, -1.64969474e-01,\n",
       "        4.03340638e-01, -1.32757977e-01, -2.97577865e-02, -8.00942555e-02,\n",
       "       -9.18094516e-02, -2.51714259e-01,  3.57957959e-01, -9.57558602e-02,\n",
       "       -1.35683298e-01, -4.79673356e-01,  1.00266293e-01, -4.00380105e-01,\n",
       "       -3.54636088e-02,  1.92604642e-02, -1.74447104e-01,  5.96494367e-03,\n",
       "       -3.54712742e-04,  1.47434011e-01,  1.29586950e-01,  1.14839777e-01,\n",
       "        4.60846066e-01, -2.15770125e-01,  2.82211006e-02, -1.41967714e-01,\n",
       "       -3.36020619e-01, -2.70528078e-01,  1.79074053e-02, -2.22296908e-01,\n",
       "        5.38783409e-02, -8.69128257e-02, -1.56356618e-01, -1.98928311e-01,\n",
       "       -4.28566150e-02,  5.65019920e-02, -1.15044108e-02,  2.97618151e-01,\n",
       "       -5.29490411e-01,  5.15494764e-01, -6.93425611e-02, -2.18229257e-02,\n",
       "        7.12868452e-01,  2.19532683e-01, -1.16361499e-01,  3.02524090e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COSINE SIMILARITY BETWEEN ALL PAGES (JUST IN THIS ONE DOC TO START)\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "page_vectors = [page.vector for page in pages_docs]\n",
    "sim = cosine_similarity(page_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99999994, 0.82639676, 0.71216565, 0.784781  , 0.9315864 ,\n",
       "        0.8354922 , 0.8292345 ],\n",
       "       [0.82639676, 0.99999994, 0.56158495, 0.55448574, 0.87179685,\n",
       "        0.85670906, 0.8897875 ],\n",
       "       [0.71216565, 0.56158495, 0.99999994, 0.8281613 , 0.6483461 ,\n",
       "        0.6298679 , 0.6682044 ],\n",
       "       [0.784781  , 0.55448574, 0.8281613 , 1.0000002 , 0.73446554,\n",
       "        0.5874811 , 0.6206418 ],\n",
       "       [0.9315864 , 0.87179685, 0.6483461 , 0.73446554, 0.9999997 ,\n",
       "        0.8175531 , 0.840251  ],\n",
       "       [0.8354922 , 0.85670906, 0.6298679 , 0.5874811 , 0.8175531 ,\n",
       "        0.99999994, 0.89020133],\n",
       "       [0.8292345 , 0.8897875 , 0.6682044 , 0.6206418 , 0.840251  ,\n",
       "        0.89020133, 1.        ]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, 1), (6, 5), (4, 0)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FIND MOST SIMILAR PAGES\n",
    "scores = pd.DataFrame(np.tril(sim, -1))\n",
    "\n",
    "k = 3 # Top 3 most similar page combinations\n",
    "\n",
    "top_k_indices = np.argpartition(np.tril(sim, -1), -k, axis=None)[-k:]\n",
    "row_indices, col_indices = np.unravel_index(top_k_indices, sim.shape)\n",
    "top_k_pagepairs = list(zip(row_indices, col_indices))\n",
    "\n",
    "top_k_pagepairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 4, 5, 6])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # FOR ALL PAGES IN THE TOP PAIRS, SPLIT INTO SENTENCES\n",
    "# wanted_page_indices = np.unique(top_k_pagepairs)\n",
    "# wanted_pages = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR ALL PAGE PAIRS, SPLIT INTO SENTENCES\n",
    "from spacy.lang.en import English\n",
    "nlp_sentencizer = English()\n",
    "nlp_sentencizer.add_pipe('sentencizer')\n",
    "\n",
    "pair_sentences = {}\n",
    "for pair in top_k_pagepairs:\n",
    "\n",
    "    # idx = 6\n",
    "    # page_text = d0.text_by_page[idx]\n",
    "    # page_sents_text = [sent.text for sent in nlp_sentencizer(page_text).sents]\n",
    "\n",
    "    pair_sentences[pair] = [\n",
    "        [sent.text for sent in nlp_sentencizer(\n",
    "            d0.text_by_page[idx]\n",
    "        ).sents]\n",
    "        for idx in pair   \n",
    "    ]\n",
    "\n",
    "    # TODO: Incorporate preliminary cleaning\n",
    "    # cutoff_characters = 30\n",
    "    # sents_text_clean = list(filter(lambda s: len(s)>cutoff_characters, sents_text))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['DoDI 1400.25-V100, December 1996 7 ENCLOSURE 2 e. Implementing procedures and programs may be issued at the operating level.',\n",
       "  'f. The DUSD(CPP) shall issue DoD Manuals as necessary to provide detailed procedural, operational, or administrative material on specific program areas or to provide model programs on subjects that should be uniform for DoD-wide application.',\n",
       "  '3.',\n",
       "  'WAIVERS.',\n",
       "  'Requests for waivers to this Volume or other DoD civilian personnel management issuances authorized by Reference (a) shall be forwarded with full justification through command channels to the DUSD(CPP) for appropriate action.'],\n",
       " ['DoDI 1400.25-V100, December 1996 2 (3) Be issued only if necessary to comply with Executive orders, law, or regulation, or to assist civilian personnel offices and human resource offices (CPOs/HROs), managers, supervisors, employees, and their representatives with civilian personnel management issues. (',\n",
       "  '4) Provide for the optimal delegation of authorities and operating responsibilities to the lowest level practical. (',\n",
       "  '5) Be distributed to all CPOs and HROs and, where practical, to managers and supervisors. (',\n",
       "  '6) Be automated to the extent practical to include automated administrative processes, decision support systems, and distribution.',\n",
       "  'c. To the maximum extent practicable, total force management should guide the design of civilian personnel policies.',\n",
       "  'Civilian personnel policies should provide unified direction by the Secretary of Defense, meet the requirements of the Combatant Commanders, and develop a shared sense of mission and responsibility among civilian employees and military personnel.',\n",
       "  'd. Civilian personnel policies, procedures, and programs as set forth in this Instruction are binding on all the DoD Components.',\n",
       "  'Existing DoD Component civilian personnel policies, procedures, and programs may continue until superseded by law, controlling regulations, new provisions of this Instruction, or related DoD issuance provisions.',\n",
       "  'e. The principles of equal employment opportunity and workforce diversity shall be incorporated into the design and implementation of civilian personnel policies, procedures, and programs at all organizational levels.',\n",
       "  'f. Consistent with workload and mission requirements, the need to create flexible work arrangements that allow employees to better balance their work and other (e.g., family) responsibilities shall be incorporated into the design and implementation of civilian personnel policies, procedures, and programs at all organizational levels.',\n",
       "  'g. DoD managers at all levels shall ensure that they satisfy any obligations to unions representing employees affected by changes to DoD policies, procedures, and programs.',\n",
       "  'Changes that conflict with existing negotiated agreements may not be implemented until the agreement expires or is renewed unless: (1) The parties agree otherwise; or (2) The change is required by law or by a rule or regulation implementing law governing prohibited personnel practices.',\n",
       "  '3.',\n",
       "  'RESPONSIBILITIES.',\n",
       "  'See Enclosure 1.']]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_sentences[(6,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n"
     ]
    }
   ],
   "source": [
    "# CREATE ALL SENTENCE PAIR COMBINATIONS\n",
    "import itertools\n",
    "# ... for pair in pair_sentences...\n",
    "sentence_combinations = list(itertools.product(*pair_sentences[(6,1)]))\n",
    "# sentence_permutations = list(itertools.permutations(sents_text, 2))\n",
    "print(len(sentence_combinations))\n",
    "# print(len(sentence_permutations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1937f5335e3c418c91b56de49c2e7cca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6370c80fac184a4e906b57ebfaab1b0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/703 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "888bee9abed94066978987df193ad1f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "838f998c366e45f5b1257e0228616e5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d561f5625d4a45b61a5b940d0ae817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc8bdbfce5014804960fae8b7edb6a38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "### LOAD CONTRADICTION MODEL\n",
    "\n",
    "# https://github.com/facebookresearch/anli/blob/main/src/hg_api/interactive.py\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "hg_model_hub_name = \"ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli\"\n",
    "\n",
    "# Will take a moment to download\n",
    "tokenizer = AutoTokenizer.from_pretrained(hg_model_hub_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(hg_model_hub_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINE CONTRADICTION FUNCTION\n",
    "def evaluate(premise, hypothesis, tokenizer=tokenizer, model=model):\n",
    "    max_length = 256\n",
    "\n",
    "    tokenized_input_seq_pair = tokenizer.encode_plus(premise, hypothesis,\n",
    "                                                     max_length=max_length,\n",
    "                                                     return_token_type_ids=True, truncation=True)\n",
    "\n",
    "    input_ids = torch.Tensor(tokenized_input_seq_pair['input_ids']).long().unsqueeze(0)\n",
    "    # remember bart doesn't have 'token_type_ids', remove the line below if you are using bart.\n",
    "    token_type_ids = torch.Tensor(tokenized_input_seq_pair['token_type_ids']).long().unsqueeze(0)\n",
    "    attention_mask = torch.Tensor(tokenized_input_seq_pair['attention_mask']).long().unsqueeze(0)\n",
    "\n",
    "    outputs = model(input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids,\n",
    "                    labels=None)\n",
    "    \n",
    "    predicted_probability = torch.softmax(outputs[0], dim=1)[0].tolist()  # batch_size only one\n",
    "\n",
    "    # Note:\n",
    "    # \"id2label\": {\n",
    "    #     \"0\": \"entailment\",\n",
    "    #     \"1\": \"neutral\",\n",
    "    #     \"2\": \"contradiction\"\n",
    "    # },\n",
    "    return predicted_probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [00:16<00:00,  4.42it/s]\n"
     ]
    }
   ],
   "source": [
    "### COMPUTE CONTRADICTION PROBABILITIES FOR ALL SENTENCE PAIRS\n",
    "# (for now this is just for one page pair)\n",
    "from tqdm import tqdm\n",
    "outputs = []\n",
    "for pair in tqdm(sentence_combinations):\n",
    "    probs = evaluate(pair[0], pair[1])\n",
    "    outputs.append(probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entailment</th>\n",
       "      <th>neutral</th>\n",
       "      <th>contradiction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.007259</td>\n",
       "      <td>0.933814</td>\n",
       "      <td>0.058927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.019067</td>\n",
       "      <td>0.909037</td>\n",
       "      <td>0.071896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002028</td>\n",
       "      <td>0.967357</td>\n",
       "      <td>0.030614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.003969</td>\n",
       "      <td>0.981052</td>\n",
       "      <td>0.014978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005850</td>\n",
       "      <td>0.965512</td>\n",
       "      <td>0.028638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   entailment   neutral  contradiction\n",
       "0    0.007259  0.933814       0.058927\n",
       "1    0.019067  0.909037       0.071896\n",
       "2    0.002028  0.967357       0.030614\n",
       "3    0.003969  0.981052       0.014978\n",
       "4    0.005850  0.965512       0.028638"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = pd.DataFrame(outputs, columns=['entailment', 'neutral', 'contradiction'])\n",
    "scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIND MOST SIMILAR PAGES\n",
    "k = 3 # top k most similar\n",
    "most_similar_idx = np.unravel_index(np.tril(sim, -1).argmax(), sim.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
