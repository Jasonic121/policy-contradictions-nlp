{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained NLI Contradiction Model\n",
    "\n",
    "Use a pre-trained model from Facebook research specifically designed to classify sentence pairs as \"Entailment\" (agree), \"Neutral\", and \"Contradiction\".\n",
    "\n",
    "https://huggingface.co/ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/facebookresearch/anli/blob/main/src/hg_api/interactive.py\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "hg_model_hub_name = \"ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli\"\n",
    "\n",
    "# Will take a moment to download\n",
    "tokenizer = AutoTokenizer.from_pretrained(hg_model_hub_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(hg_model_hub_name)\n",
    "\n",
    "def evaluate(tokenizer, model, premise, hypothesis):\n",
    "    max_length = 256\n",
    "\n",
    "    tokenized_input_seq_pair = tokenizer.encode_plus(premise, hypothesis,\n",
    "                                                     max_length=max_length,\n",
    "                                                     return_token_type_ids=True, truncation=True)\n",
    "\n",
    "    input_ids = torch.Tensor(tokenized_input_seq_pair['input_ids']).long().unsqueeze(0)\n",
    "    # remember bart doesn't have 'token_type_ids', remove the line below if you are using bart.\n",
    "    token_type_ids = torch.Tensor(tokenized_input_seq_pair['token_type_ids']).long().unsqueeze(0)\n",
    "    attention_mask = torch.Tensor(tokenized_input_seq_pair['attention_mask']).long().unsqueeze(0)\n",
    "\n",
    "    outputs = model(input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids,\n",
    "                    labels=None)\n",
    "    # Note:\n",
    "    # \"id2label\": {\n",
    "    #     \"0\": \"entailment\",\n",
    "    #     \"1\": \"neutral\",\n",
    "    #     \"2\": \"contradiction\"\n",
    "    # },\n",
    "\n",
    "    predicted_probability = torch.softmax(outputs[0], dim=1)[0].tolist()  # batch_size only one\n",
    "\n",
    "    #print(\"Premise:\", premise)\n",
    "    #print(\"Hypothesis:\", hypothesis)\n",
    "    print(\"Prediction:\")\n",
    "    print(\"Entailment:\", predicted_probability[0])\n",
    "    print(\"Neutral:\", predicted_probability[1])\n",
    "    print(\"Contradiction:\", predicted_probability[2])\n",
    "\n",
    "    print(\"=\"*20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Contradiction] Prediction:\n",
      "Entailment: 0.005922275595366955\n",
      "Neutral: 0.5431420207023621\n",
      "Contradiction: 0.4509357511997223\n",
      "====================\n",
      "[Entailment] Prediction:\n",
      "Entailment: 0.16487528383731842\n",
      "Neutral: 0.5480648279190063\n",
      "Contradiction: 0.2870599031448364\n",
      "====================\n",
      "[Neutral] Prediction:\n",
      "Entailment: 0.12228141725063324\n",
      "Neutral: 0.6374253630638123\n",
      "Contradiction: 0.2402932494878769\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "premise = \"The office is responsible for writing a report on the project status within 6 months.\"\n",
    "hypothesis_contradict = \"Interim project status reports are not required.\"\n",
    "hypothesis_entail = \"Interim project status reports will be required.\"\n",
    "hypothesis_neutral = \"Operationsâ€”activities or processes associated with the programs to be housed in a completed facility and those processes which are necessary to run the facility.\"\n",
    "\n",
    "print(\"[Contradiction]\", end=' '); evaluate(tokenizer, model, premise, hypothesis_contradict)\n",
    "print(\"[Entailment]\", end=' '); evaluate(tokenizer, model, premise, hypothesis_entail)\n",
    "print(\"[Neutral]\", end=' '); evaluate(tokenizer, model, premise, hypothesis_neutral)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Initial observations and thoughts**\n",
    "> \n",
    "> Note that this is based on literally only a single example. But, here we go:\n",
    "> * For our problem statement, we don't necessarily care about distinction between *entailment* and *neutral*. We really just want to catch *contradiction*\n",
    "> * Notice for the contradictory statement, the entailment probability was extremely low! And the contradiction probability was much higher than for the entailment and neutral statements.\n",
    "> * Ideally we'd be able to fine-tune these models on our specific vocab and use-case...\n",
    ">   * Perhaps we can train an estimator to take these output probabilities and produce a fine-tuned binary classification of contradiction or not\n",
    ">   * We'd need a labeled training set\n",
    "> * For now, we can just do some stats and create a custom heuristic for our contradiction classification!\n",
    "> * Named Entities will likely mess us up a bit, since policies often use definitions and so two policies may say \"The Headquarters\" and be referring to two different headquarters!\n",
    "> * I like this approach to get us started. We could also try the much more informed policy-parsing approach, maybe trying to use some existing Deloitte models/assets, such as RegExplorer..?\n",
    "> * Next step is to run this inference on pairwise sentences across documents\n",
    ">   * Might take a while since pairwise is exponential... I wonder what is `tokenizer.encode_plus` doing? Could we run tokenization ahead of time and save the results to a file?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
